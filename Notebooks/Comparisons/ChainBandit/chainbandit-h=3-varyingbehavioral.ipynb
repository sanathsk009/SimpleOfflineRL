{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0a76d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from OfflineSRL.MDPDataset.old_dataset import *\n",
    "from OfflineSRL.MDP.old_MDP import MDP\n",
    "from OfflineSRL.MDP.ChainBandit import ChainBanditMDP\n",
    "from OfflineSRL.MDP.ChainBanditState import ChainBanditState\n",
    "from OfflineSRL.BPolicy.ChainBanditPolicy import ChainBanditPolicy\n",
    "from OfflineSRL.OfflineLearners.offlineLearners import VI, PVI, SPVI, PesBandit\n",
    "from OfflineSRL.OfflineEvaluators.offlineEvaluator import SPVIEval, StandardPesEval\n",
    "\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c7302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(horizon = 3, neps = 50, third_action_prob = 0.2, num_states =10):\n",
    "    # Initialize MDP and Policy\n",
    "    mdp = ChainBanditMDP(num_states = num_states)\n",
    "    policy = ChainBanditPolicy(mdp, third_action_prob = third_action_prob)\n",
    "    \n",
    "    # Generate data\n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    terminals = []\n",
    "    for eps in range(neps):\n",
    "        for timestep in range(horizon+2):\n",
    "            # Get state.\n",
    "            # Add state to list.\n",
    "            cur_state = copy.deepcopy(mdp.cur_state)\n",
    "            print(str(cur_state) + \"CURRENT_STATE ------------------------------------\")\n",
    "            observations.append(copy.deepcopy(cur_state.num_list))\n",
    "\n",
    "            # Get action\n",
    "            # Add action to list\n",
    "            cur_action = policy._get_action(state = cur_state)\n",
    "            actions.append(copy.deepcopy(cur_action))\n",
    "\n",
    "            # Execute action\n",
    "            reward, next_state = mdp.execute_agent_action(cur_action)\n",
    "            # Add reward\n",
    "            rewards.append(copy.deepcopy(reward))\n",
    "\n",
    "            terminals.append(0)\n",
    "        mdp.reset()\n",
    "        terminals[-1] = 1\n",
    "        \n",
    "    # Convert to MDPDataset format\n",
    "    observations = np.array(observations)\n",
    "    actions = np.array(actions)\n",
    "    rewards = np.array(rewards)\n",
    "    terminals = np.array(terminals)\n",
    "\n",
    "    dataset = MDPDataset(\n",
    "        observations=observations,\n",
    "        actions=actions,\n",
    "        rewards=rewards,\n",
    "        terminals=terminals,\n",
    "    )\n",
    "    \n",
    "    return observations, policy, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b051d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_learner(option, observations, policy, dataset, horizon, neps = 5000, num_states =10):\n",
    "    max_step_reward = 1\n",
    "    abs_max_ep_reward = 1\n",
    "    min_step_reward = 0\n",
    "    if option == \"VI\":\n",
    "        vi = VI(name = \"vi\", states = observations, actions = policy.actions, epLen = horizon,is_eval = False)\n",
    "    if option == \"PVI\":\n",
    "        vi = PVI(name = \"pvi\", states = observations, actions = policy.actions, epLen = horizon, is_eval = False,\n",
    "                 max_step_reward = max_step_reward, min_step_reward = min_step_reward, abs_max_ep_reward = abs_max_ep_reward)\n",
    "    if option == \"SPVI\":\n",
    "        vi = SPVI(name = \"spvi\", states = observations, actions = policy.actions, epLen = horizon, bpolicy = policy, is_eval = False,\n",
    "                  max_step_reward = max_step_reward, min_step_reward = min_step_reward, abs_max_ep_reward = abs_max_ep_reward)\n",
    "    if option == \"PSL\":\n",
    "        vi = PesBandit(name = \"psl\", states = observations, actions = policy.actions, epLen = horizon, is_eval = False)\n",
    "\n",
    "    vi.fit(dataset)\n",
    "    eval_policy = {}\n",
    "    # eval_policy\n",
    "    for timestep in range(horizon):\n",
    "        for s in range(vi.n_states):\n",
    "            eval_policy[s, timestep] = vi.agent.action_prob(s, timestep)\n",
    "    final_policy = {}\n",
    "\n",
    "    uncertainty_vec_spvi = []\n",
    "    uncertainty_vec_pvi = []\n",
    "    alphas = [0]\n",
    "    # neps = [10000, 20000, 50000]\n",
    "\n",
    "  \n",
    "    for timestep in range(horizon):\n",
    "        for s in range(vi.n_states):\n",
    "            # final_policy[s, timestep] = eval_policy[s, timestep]*alpha + (1-alpha)*(np.array(policy._get_probs(s, timestep)))\n",
    "            final_policy[s, timestep] = (np.array(policy._get_probs(s, timestep)))\n",
    "\n",
    "\n",
    "    pvi_eval = StandardPesEval(name = \"pvi\", states = num_states, actions=policy.actions, epLen=horizon, evalpolicy=final_policy, is_eval = True)\n",
    "    vi_eval = SPVIEval(name = \"spvi\", states = num_states, actions = policy.actions, epLen = horizon, bpolicy = policy,evalpolicy=final_policy,data_splitting=0, delta=0.9, nTrajectories=neps, epsilon1=0.01, epsilon2=0.01, epsilon3=0.01,\n",
    "                max_step_reward = max_step_reward, min_step_reward = min_step_reward, abs_max_ep_reward = abs_max_ep_reward, is_eval = True)\n",
    "\n",
    "    pvi_eval.fit(dataset)\n",
    "    vi_eval.fit(dataset)\n",
    "\n",
    "    uncertainty_vec_pvi.append(pvi_eval.agent.get_interval())\n",
    "    uncertainty_vec_spvi.append(vi_eval.agent.get_interval())\n",
    "    \n",
    "    return (pvi_eval.agent.get_interval(), vi_eval.agent.get_interval())\n",
    "    # mdp = ChainBanditMDP(num_states = horizon)\n",
    "    # viobservations = []\n",
    "    # viactions = []\n",
    "    # virewards = []\n",
    "    # viterminals = []\n",
    "    # for eps in range(neps):\n",
    "    #     for timestep in range(horizon):\n",
    "    #         # Get state.\n",
    "    #         # Add state to list.\n",
    "    #         cur_state = copy.deepcopy(mdp.cur_state)\n",
    "    #         viobservations.append(copy.deepcopy(cur_state.num_list))\n",
    "\n",
    "    #         # Get action\n",
    "    #         # Add action to list\n",
    "    #         cur_action = vi.act(copy.deepcopy(cur_state.num_list), timestep)\n",
    "    #         viactions.append(copy.deepcopy(cur_action))\n",
    "\n",
    "    #         # Execute action\n",
    "    #         reward, next_state = mdp.execute_agent_action(cur_action)\n",
    "    #         # Add reward\n",
    "    #         virewards.append(copy.deepcopy(reward))\n",
    "\n",
    "    #         viterminals.append(0)\n",
    "    #     mdp.reset()\n",
    "    #     viterminals[-1] = 1\n",
    "    # return np.sum(np.array(virewards))/neps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b5762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rew_dict = {}\n",
    "option_list = [\"PSL\",\"PVI\",\"SPVI\"]\n",
    "# for option in option_list:\n",
    "#     rew_dict[option] = {}\n",
    "n_runs = 1\n",
    "horizon = 100\n",
    "neps_list = [4000, 6000, 8000]\n",
    "\n",
    "pvi_vec = []\n",
    "spvi_vec = []\n",
    "num_states = 25\n",
    "# for neps in neps_list:\n",
    "#     print(neps)\n",
    "#     # for option in option_list:\n",
    "#     #     rew_dict[option][neps] = []\n",
    "#     rew_dict[neps] = []\n",
    "#     for run in range(n_runs):\n",
    "#         observations, policy, dataset = get_dataset(horizon = horizon, neps = neps, third_action_prob = 0.8)\n",
    "#         # for option in option_list:\n",
    "#         rew_dict[neps].append(evaluate_learner(\"SPVI\", copy.deepcopy(observations), policy, dataset, horizon, neps))\n",
    "            #print(option)\n",
    "            #print(option, neps, evaluate_learner(option, copy.deepcopy(observations), policy, dataset, horizon))\n",
    "for neps in neps_list:\n",
    "    observations, policy, dataset = get_dataset(horizon = horizon, neps = neps, third_action_prob = 0.2, num_states = num_states)\n",
    "\n",
    "    l, p = evaluate_learner(\"SPVI\", copy.deepcopy(observations), policy, dataset, horizon, neps, num_states)\n",
    "    pvi_vec.append(l)\n",
    "    spvi_vec.append(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9758c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rew = {}\n",
    "# err = {}\n",
    "# bounds = []\n",
    "# # for option in option_list:\n",
    "# #     rew[option] = []\n",
    "# #     err[option] = []\n",
    "# for neps in neps_list:\n",
    "#     # for option in option_list:\n",
    "#         bounds.append(np.mean(rew_dict[neps]))\n",
    "        # err[option].append(np.std(rew_dict[option][neps])/np.sqrt(n_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd755ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# if using a Jupyter notebook, include:\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# for option in option_list:\n",
    "x = [2000, 4000, 6000]\n",
    "y = pvi_vec\n",
    "y1 = spvi_vec\n",
    "# yerr = err[option]\n",
    "# ax.errorbar(x, y)\n",
    "#             # yerr=yerr,\n",
    "#             # fmt='-o', label = option)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(x,y1 )\n",
    "print(x)\n",
    "print(y)\n",
    "# ax.set_xlabel('Number of training episodes')\n",
    "# ax.set_ylabel('Test reward')\n",
    "# ax.set_title('Chain Bandit: horizon = '+str(horizon))\n",
    "# plt.legend()\n",
    "\n",
    "# plt.savefig('chainbandit-h=3-suopt=0.8.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29519bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for option in option_list:\n",
    "    x = neps_list\n",
    "    y = np.array(rew[option])\n",
    "    yerr = np.array(err[option])\n",
    "    ax.plot(x, y, '-o', label = option)\n",
    "    ax.fill_between(x, y-yerr, y+yerr)\n",
    "    #ax.errorbar(x, y,\n",
    "    #            yerr=yerr,\n",
    "    #            fmt='-o', label = option)\n",
    "\n",
    "\n",
    "ax.set_xlabel('Number of training episodes')\n",
    "ax.set_ylabel('Test reward')\n",
    "ax.set_title('Chain Bandit: horizon = '+str(horizon))\n",
    "plt.legend()\n",
    "\n",
    "#x = np.linspace(0, 30, 30)\n",
    "#y = np.sin(x/6*np.pi)\n",
    "#error = np.random.normal(0.1, 0.02, size=y.shape)\n",
    "#y += np.random.normal(0, 0.1, size=y.shape)\n",
    "\n",
    "#plt.plot(x, y, 'k-')\n",
    "#plt.fill_between(x, y-error, y+error)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fbbe51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('offlineSRL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a114b775e0518fb067d9e7998144bb165de7a99216eb0e97954423fbcb2be81e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
